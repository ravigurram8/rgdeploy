AWSTemplateFormatVersion: '2010-09-09'

Description: 'AWS CloudFormation Sample Template Sample template EIP_With_Association:
  This template shows how to associate an Elastic IP address with an Amazon EC2 instance
  - you can use this same technique to associate an EC2 instance with an Elastic IP
  Address that is not created inside the template by replacing the EIP reference in
  the AWS::EC2::EIPAssoication resource type with the IP address of the external EIP.
  **WARNING** This template creates an Amazon EC2 instance and an Elastic IP Address.
  You will be billed for the AWS resources used if you create a stack from this template.'

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Nextflow Configuration
        Parameters:
          - PipelineName
          - PipelineContainer
          - InputDataLocation
          - InputDataPattern 
          - OutputDataLocation
      - Label:
          default: Head Node Configuration
        Parameters:
          - InstanceType
          - HeadNodeEBSVolumeSize
          - KeyPair
          - AllowedSSHLocation
      - Label:
          default: Batch Configuration
        Parameters:
          - VpcId
          - WorkerNodeSubnetId
          - ComputeEnvMinvCpus
          - ComputeEnvMaxvCpus
          - SpotBidPercentage
          - WorkerNodeInstanceType
          - WorkerNodeEBSVolumeSize
          - ResearcherName
          - ProjectId
    ALB: true
    ProductName: nextflow

Parameters:
  VpcId:
    Type: AWS::EC2::VPC::Id
    Description: 'The VPC to create security groups and deploy AWS Batch to.'
  WorkerNodeSubnetId:
    Type: AWS::EC2::Subnet::Id
    Description: 'Subnet you want your Batch Worker Node to launch in. We recommend public subnets.'
  HeadNodeEBSVolumeSize:
    Description: The initial size of the volume (in GBs) Head Node EBS will use for storage.
    Type: Number
    Default: 16
  PipelineContainer:
    Type: String
    Description: Public Docker container image of the pipeline to be executed. If you are using a custom pipeline, ensure that the custom container image is publicly available on Docker Hub
  InputDataLocation:
    Description: >-
      The path to the location which holds the Input data files. This can point to a local path on the head node or to an S3 bucket. Eg /home/ec2-user/studies/ProjectStorage/Shared or mys3bucket.   
      Type in the search box to find s3 buckets created through Research Gateway.
    Type: String
  InputDataPattern:
    Description: >-
      The pattern to match samples to be processed as inputs to the pipeline. E.g. data/ggal/*_{1,2}.fq. It can also point to a csv or tsv file that contains details of the files to be processed.
    Type: String
    Default: samplesheet.csv 
  OutputDataLocation:
    Description: >-
      The path to the location which holds the Output data files. This can point to a local path on the head node or to an S3 bucket. 
      A local path should be accessible to the user ec2-user. Alternately, provide an S3 bucket for storing analysis results. 
      Type in the search box to find s3 buckets created through Research Gateway.
    Default: /home/ec2-user/nextflow/outputs/  
    Type: String
  ComputeEnvMinvCpus:
    Description: The minimum number of CPUs to be kept in running state for the Batch Worker Nodes. If you give a non-zero value, some worker nodes may stay in running state always and you may incur higher cost.
    Type: Number
    Default: 0
  ComputeEnvMaxvCpus:
    Description: The maximum number of CPUs for the default Batch Compute Environment
    Type: Number
    Default: 100
  SpotBidPercentage:
    Type: Number
    Description: The maximum percentage of On-Demand pricing you want to pay for Spot resources. You will always pay the lowest Spot market price and never more than your maximum percentage.
    Default: 100
  InstanceType:
    Description: Head Node EC2 instance type
    Type: String
    Default: t3.small
    AllowedValues: [t3.nano, t3.micro, t3.small, t3.medium]
    ConstraintDescription: Must be a valid EC2 instance type.
  KeyPair:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the Head Node. If no key pairs exist, please create one from the button next to the dropdown. Please contact your Administrator if you are unable to create one.
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: Must be the name of an existing EC2 KeyPair.
  AllowedSSHLocation:
    Description: The IP address range that can be used to SSH to the Head Node
    Type: String
    MinLength: '9'
    MaxLength: '18'
    Default: 0.0.0.0/0
    AllowedPattern: (\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})/(\d{1,2})
    ConstraintDescription: Must be a valid IP CIDR range of the form x.x.x.x/x.
  PipelineName:
    Type: String
    Default: https://github.com/seqeralabs/nextflow-tutorial.git
    Description: Search and select the pipeline git repository URL. If not found please enter the custom pipeline URL. The repo should contain the nextflow.config file which specifies the name of the docker container image.
  WorkerNodeInstanceType:
    Description: Specify the instance types to be used to carry out the computation. You can specify one or more family or instance type. The option 'optimal' chooses the best fit of M4, C4, and R4 instance types available in the region.
    Type: String
    Default: optimal
    AllowedValues: [optimal, c4.large, m4.large, r4.large, c4.4xlarge, m4.4xlarge, r4.4xlarge]
  WorkerNodeEBSVolumeSize:
    Description: The initial size of the volume (in GBs) Worker Node EBS will use for storage.
    Type: Number
    Default: 100
  ResearcherName:
    Type: String
  ProjectId:
    Type: String
  Namespace:
    Type: String
    Description: An environment name that will be prefixed to resource names
  S3Mounts:
    Type: String
    Description: A JSON array of objects with name, bucket, and prefix properties used to mount data
  IamPolicyDocument:
    Type: String
    Description: The IAM policy to be associated with the launched workstation
  EnvironmentInstanceFiles:
    Type: String
    Description: >-
      An S3 URI (starting with "s3://") that specifies the location of files to be copied to
      the environment instance, including any bootstrap scripts

Conditions:
  IamPolicyEmpty: !Equals [!Ref IamPolicyDocument, '{}']

Resources:
  IamInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - Ref: EcsInstanceRole
  
  EcsInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Join ['-', [Ref: Namespace, 'nextflow-head-role']]
      AssumeRolePolicyDocument:
        Version: '2008-10-17'
        Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - !If
          - IamPolicyEmpty
          - !Ref 'AWS::NoValue'
          - PolicyName: !Join ['-', [Ref: Namespace, 's3-studydata-policy']]
            PolicyDocument: !Ref IamPolicyDocument
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
        - arn:aws:iam::aws:policy/AWSBatchFullAccess
        - arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPowerUser
        - arn:aws:iam::aws:policy/AmazonElasticContainerRegistryPublicPowerUser
        - arn:aws:iam::aws:policy/AmazonSSMFullAccess
  
  EC2LaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateName:
        !Sub
            - 'nf-launch-tp-${RandomLTGUID}'
            - { RandomLTGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }      
      LaunchTemplateData:
        # Used in tandem with UserData to check if the instance is provisioned
        # correctly. It is important to terminate mis-provisioned instances before
        # jobs are placed on them
        InstanceInitiatedShutdownBehavior: terminate
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: cost_resource
                Value: !Sub ${AWS::StackName}
              - Key: Name
                Value: !Sub worker-node-${AWS::StackName}
              - Key: researcher_name
                Value: !Ref ResearcherName
              - Key: project_name
                Value: !Ref ProjectId
          - ResourceType: volume
            Tags:
              - Key: cost_resource
                Value: !Sub ${AWS::StackName}
              - Key: Name
                Value: !Sub worker-node-${AWS::StackName}
              - Key: researcher_name
                Value: !Ref ResearcherName
              - Key: project_name
                Value: !Ref ProjectId
        BlockDeviceMappings:
          - Ebs:
              DeleteOnTermination: True
              VolumeSize: !Ref WorkerNodeEBSVolumeSize
              VolumeType: gp2
            DeviceName: /dev/xvda
          - Ebs:
              Encrypted: True
              DeleteOnTermination: True
              VolumeSize: 22
              VolumeType: gp2
            DeviceName: /dev/xvdcz
          - Ebs:
              Encrypted: True
              DeleteOnTermination: True
              VolumeSize: 100
              VolumeType: gp2
            DeviceName: /dev/xvdba
        EbsOptimized: true
        UserData:
          Fn::Base64:
            Fn::Sub: |
              MIME-Version: 1.0
              Content-Type: multipart/mixed; boundary="==BOUNDARY=="

              --==BOUNDARY==
              Content-Type: text/cloud-config; charset="us-ascii"

              #cloud-config
              repo_update: true
              repo_upgrade: security

              packages:
              - jq
              - btrfs-progs
              - sed
              - git
              - amazon-ssm-agent
              - unzip
              - amazon-cloudwatch-agent

              runcmd:

              # install AWS CLI with Miniconda:
              - sudo yum install -y bzip2 wget
              - wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
              - bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /home/ec2-user/miniconda
              - /home/ec2-user/miniconda/bin/conda install -c conda-forge -y awscli
              - rm Miniconda3-latest-Linux-x86_64.sh

              --==BOUNDARY==--
  
  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: SG for nextflow workflows on Batch
      VpcId:
        Ref: VpcId
  
  S3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
    UpdateReplacePolicy: Delete
    Properties:
      BucketName:
        !Sub
            - 'nextflow-work-data-${RandomGUID}'
            - { RandomGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
             SSEAlgorithm: AES256
      Tags:
        - Key: cost_resource
          Value: !Sub ${AWS::StackName}
        - Key: Name
          Value: !Sub S3-worker-dir-${AWS::StackName} 
          
  S3CleanupLambdaInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "lambda.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: !Sub S3Bucket-Access-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Sid: S3BucketAllowAllObjectOps
                Effect: Allow
                Resource: 
                  - !Join ["", ["arn:aws:s3:::", !Ref S3Bucket]]
                  - !Join ["", ["arn:aws:s3:::", !Ref S3Bucket, "/*"]]
                Action:
                  - "s3:*"

  S3CleanupLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code: 
        ZipFile: 
          !Sub |
            import json, boto3, logging
            import cfnresponse
            logger = logging.getLogger()
            logger.setLevel(logging.INFO)

            def lambda_handler(event, context):
                logger.info("event: {}".format(event))
                try:
                    bucket = event['ResourceProperties']['BucketName']
                    logger.info("bucket: {}, event['RequestType']: {}".format(bucket,event['RequestType']))
                    if event['RequestType'] == 'Delete':
                        s3 = boto3.resource('s3')
                        bucket = s3.Bucket(bucket)
                        for obj in bucket.objects.filter():
                            logger.info("delete obj: {}".format(obj))
                            s3.Object(bucket.name, obj.key).delete()

                    sendResponseCfn(event, context, cfnresponse.SUCCESS)
                except Exception as e:
                    logger.info("Exception: {}".format(e))
                    sendResponseCfn(event, context, cfnresponse.FAILED)

            def sendResponseCfn(event, context, responseStatus):
                responseData = {}
                responseData['Data'] = {}
                cfnresponse.send(event, context, responseStatus, responseData, "CustomResourcePhysicalID")            

      Handler: "index.lambda_handler"
      Runtime: python3.7
      MemorySize: 128
      Timeout: 900
      Role: !GetAtt S3CleanupLambdaInstanceRole.Arn
    DependsOn: S3CleanupLambdaInstanceRole  
  
  S3CleanUpBucketOnDelete:
    Type: Custom::cleanupbucket
    Properties:
      ServiceToken: !GetAtt S3CleanupLambdaFunction.Arn
      BucketName: !Ref S3Bucket
    DependsOn: S3Bucket

  BatchJobRole:
    Type: AWS::IAM::Role
    Properties:
      Policies:
        - PolicyName: !Sub S3Bucket-Access-${AWS::StackName}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Deny
                Resource: !Join ["", ["arn:aws:s3:::", !Ref S3Bucket]]
                Action:
                  - "s3:Delete*"
                  - "s3:PutBucket*"
              - Effect: Allow
                Resource: !Join ["", ["arn:aws:s3:::", !Ref S3Bucket]]
                Action:
                  - "s3:ListBucket*"
              - Effect: Allow
                Resource: !Join ["", ["arn:aws:s3:::", !Ref S3Bucket, "/*"]]
                Action:
                  - "s3:*"
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "ecs-tasks.amazonaws.com"
            Action:
              - "sts:AssumeRole"
  
  BatchInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "ec2.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: !Sub S3Bucket-Access-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Sid: S3BucketAllowAllObjectOps
                Effect: Allow
                Resource: 
                  - !Join ["", ["arn:aws:s3:::", !Ref S3Bucket]]
                  - !Join ["", ["arn:aws:s3:::", !Ref S3Bucket, "/*"]]
                Action:
                  - "s3:*"
 
        # required for amazon-ebs-autoscale to resize filesystems
        - PolicyName: !Sub Autoscale-EBS-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              Effect: Allow
              Action:
                - "ec2:createTags"
                - "ec2:createVolume"
                - "ec2:attachVolume"
                - "ec2:deleteVolume"
                - "ec2:modifyInstanceAttribute"
                - "ec2:describeVolumes"
              Resource: "*"
        
        # enable retrieving provisioning scripts from codecommit
        - PolicyName: !Sub CodeCommit-Access-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              Effect: Allow
              Action:
                - codecommit:GitPull
              Resource:
                - !Sub arn:aws:codecommit:*:${AWS::AccountId}:*-${AWS::StackId}
        
        # enable reading SSM parameters by provisioning scripts
        - PolicyName: !Sub SystemsManager-Access-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - ssm:GetParameter*
                Resource:
                  - !Sub arn:aws:ssm:*:${AWS::AccountId}:parameter/rl/${AWS::StackId}/*
              - Effect: Allow
                Action:
                  - ssm:DescribeParameters
                Resource: "*"
                
      ManagedPolicyArns:
      - "arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role"
      - "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
      - "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
      - "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy"
  
  BatchInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - Ref: BatchInstanceRole
  
  BatchSpotFleetRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "spotfleet.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      ManagedPolicyArns:
      - "arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole"
  
  BatchServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: batch.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole

  SpotComputeEnv:
    Type: AWS::Batch::ComputeEnvironment
    Properties:
      ComputeEnvironmentName: 
        !Sub
            - 'nextflow-work-spot-${RandomGUID}'
            - { RandomGUID: !Select [0, !Split ["-", !Select [2, !Split ["/", !Ref AWS::StackId ]]]] }
      ServiceRole: !GetAtt BatchServiceRole.Arn
      Type: MANAGED
      State: ENABLED
      ComputeResources:
        Ec2Configuration:
          - ImageType: ECS_AL2
        AllocationStrategy: SPOT_CAPACITY_OPTIMIZED
        # Set the Spot price to 100% of on-demand price
        # This is the maximum price for spot instances that Batch will launch.
        # Lowering this puts a limit on the spot capacity that Batch has available.
        # Spot instances are terminated when on-demand capacity is needed, regardless of the price set.
        BidPercentage: !Ref SpotBidPercentage
        # Ec2KeyPair: !Ref Ec2KeyPairName
        LaunchTemplate:
          LaunchTemplateId: !Ref EC2LaunchTemplate
          Version: $Latest
        InstanceRole: !GetAtt BatchInstanceProfile.Arn
        InstanceTypes: 
          - !Ref WorkerNodeInstanceType
        MinvCpus: !Ref ComputeEnvMinvCpus
        MaxvCpus: !Ref ComputeEnvMaxvCpus
        SecurityGroupIds:
          - !Ref SecurityGroup
        SpotIamFleetRole: !GetAtt BatchSpotFleetRole.Arn
        Subnets:
          - !Ref WorkerNodeSubnetId
        Type: SPOT
        Tags:
          Name: !Sub worker-node-${AWS::StackName} 

  DefaultQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      Tags: 
        Name: !Sub worker-node-queue-${Namespace}
      JobQueueName: !Sub worker-node-queue-${Namespace}
      Priority: 1
      State: ENABLED
      ComputeEnvironmentOrder:
        - Order: 1
          ComputeEnvironment: !Ref SpotComputeEnv

  EC2Instance:
    Type: AWS::EC2::Instance
    CreationPolicy:
      ResourceSignal:
        Timeout: PT10M
    Properties:
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash -xe
          exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1
          
          cd /usr/bin
          sudo wget https://download.oracle.com/java/17/latest/jdk-17_linux-x64_bin.rpm
          sudo rpm -ivh jdk-17_linux-x64_bin.rpm
          sudo wget -qO- https://github.com/nextflow-io/nextflow/releases/download/v21.11.0-edge/nextflow-21.11.0-edge-all | sudo bash

          sudo yum update -y --security
          cd /root/ 
          
          pattern=${InputDataPattern}
          pattern=$(echo $pattern | sed -e 's#^/*##')
          echo $pattern

          getOutputData=${OutputDataLocation}
          getFirstChar=`echo $getOutputData | cut -c1-1`
            
          if [ "$getFirstChar" == "/" ] 
          then :
              echo "Ngnix URL"
              OutputDataLocation=${OutputDataLocation}
              echo $OutputDataLocation
          else
              echo "S3 Path"
              OutputDataLocation="s3://"${OutputDataLocation}
              echo $OutputDataLocation 
          fi


          getInputData=${InputDataLocation}
          getIFirstChar=`echo $getInputData | cut -c1-1`
            
          if [ "$getIFirstChar" == "/" ] 
          then :
              echo "Ngnix URL"
              InputDataLocation=${InputDataLocation}
              echo $InputDataLocation
          else
              echo "S3 Path"
              InputDataLocation="s3://"${InputDataLocation}
              echo $InputDataLocation 
          fi

          sudo chown -R ec2-user:ec2-user /usr/bin/nextflow
          sudo cp -prf /root/.nextflow/ /home/ec2-user/ 
          cd /home/ec2-user/.nextflow/
          
          PROJECT_NF_CONFIG=config 

          cat << EOF >> $PROJECT_NF_CONFIG

          process.container = '${PipelineContainer}'
          plugins {
            id 'nf-amazon'
          }
          docker.enabled = true
          params.outdir='$OutputDataLocation'
          params.reads='$InputDataLocation/$pattern'
          params.input='$InputDataLocation/$pattern'
          profiles {
            standard {
              process.container = '${PipelineContainer}'
              docker.enabled = true
            }

            batch {
              process.container = '${PipelineContainer}'
              process.executor = 'awsbatch'
              process.queue = '${DefaultQueue}'
              workDir = 's3://${S3Bucket}/${AWS::StackName}'
              process.errorStrategy = {                                                       
                sleep( Math.pow( 2, task.attempt ) * 150 as long )                    
                return 'retry'                                                        
              }                                                                       
              process.maxRetries = 5
              aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'
              aws.batch.maxParallelTransfers = 5
              aws.batch.maxTransferAttempts = 5
            }
          }

          EOF
          
          cd /home/ec2-user/
          git clone ${PipelineName} pipeline
          cd pipeline

          echo ${PipelineName} > PipelineRepo.TXT 

          NEXTFLOW_CONFIG=nextflow.config
          
          cat << EOF >> $NEXTFLOW_CONFIG

          params.outdir='$OutputDataLocation'
          
          EOF
  
          docker pull ${PipelineContainer}
          chown -R ec2-user:ec2-user /home/ec2-user/
          chown -R ec2-user:ec2-user /home/ec2-user/.[^.]*

          usermod -aG docker ec2-user
          gpasswd -a ec2-user docker
          systemctl restart docker

          #Download and execute bootstrap script
          aws s3 cp "${EnvironmentInstanceFiles}/get_bootstrap.sh" "/tmp"
          chmod 500 "/tmp/get_bootstrap.sh"
          /tmp/get_bootstrap.sh "${EnvironmentInstanceFiles}" '${S3Mounts}'
          /opt/aws/bin/cfn-signal --exit-code 0 --resource EC2Instance --region ${AWS::Region} --stack ${AWS::StackName}
 
      InstanceType: !Ref 'InstanceType'
      SecurityGroupIds:
        - { "Fn::GetAtt" : ["InstanceSecurityGroup", "GroupId"] }
      KeyName: !Ref 'KeyPair'
      ImageId: '{{resolve:ssm:/RL/RG/StandardCatalog/nextflow-latest}}'
      IamInstanceProfile: !Ref IamInstanceProfile
      PropagateTagsToVolumeOnCreation: true
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: !Ref HeadNodeEBSVolumeSize
            Encrypted: true
      Tags:
        - Key: Name
          Value: !Join ['-', [Ref: Namespace, 'nextflow-head-node']]
        - Key: cost_resource
          Value: !Ref "AWS::StackName"
        - Key: Description
          Value: EC2 workspace instance
  
  InstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access
      VpcId: !Ref VpcId
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: '22'
          ToPort: '22'
          CidrIp: !Ref 'AllowedSSHLocation'
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          CidrIp: !Ref 'AllowedSSHLocation'
        - IpProtocol: tcp
          FromPort: 8000
          ToPort: 8000
          CidrIp: !Ref 'AllowedSSHLocation'
        - IpProtocol: tcp
          FromPort: 443
          ToPort: 443
          CidrIp: !Ref 'AllowedSSHLocation'

Outputs:
  InstanceId:
    Description: InstanceId of the newly created EC2 instance
    Value: !Ref 'EC2Instance'
  InstanceIPAddress:
    Description: IP address of the newly created EC2 instance
    Value: !GetAtt [EC2Instance, PublicIp]
  InstanceDNSName:
    Description: DNS name of the newly created EC2 instance
    Value: !GetAtt [EC2Instance, PublicDnsName]
  MetaConnection1Name:
    Description: Name for connection
    Value: SSH to Main EC2 instance
  MetaConnection1InstanceId:
    Description: EC2 Linux Instance Id
    Value: !Ref EC2Instance
  MetaConnection1Scheme:
    Description: Protocol for connection
    Value: ssh
  Ec2WorkspaceInstanceId:
    Description: Instance Id for the EC2 workspace instance
    Value: !Ref EC2Instance
  LaunchTemplateId:
    Description: >-
      EC2 Launch Template ID to use when creating AWS Batch compute environments
    Value: !Ref EC2LaunchTemplate
  WorkDataLocation:
    Value: !Sub ${S3Bucket}
  BatchQueue:
    Value: !Sub ${DefaultQueue}
  BatchComputeEnvironment:
    Value: !Ref SpotComputeEnv
  AvailabilityZone: 
    Description: AvailabilityZone of newly created EC2 instance
    Value: !GetAtt [EC2Instance, AvailabilityZone]  
